\documentclass{beamer}

\usepackage[francais]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{enumitem}
\usepackage{xcolor}
\usepackage{qtree}
\usepackage{amsthm}
\usepackage{multimedia}
\usepackage[mathscr]{eucal}

\title{Analyse syntaxique en constituants}

\subtitle{Impl\'ementation d'un analyseur syntaxique probabiliste}

\author{Kira KIRANOVA, Antoine LAFOUASSE, Yulia MATVEEVA}

% - Use the \inst command only if there are several affiliations.
% - Keep it simple, no one is interested in your street address.
\institute{Universit\'e Paris Diderot --- Paris 7}

\date{25 mars 2013}

% If you have a file called "university-logo-filename.xxx", where xxx
% is a graphic format that can be processed by latex or pdflatex,
% resp., then you can add a logo as follows:

\pgfdeclareimage[height=0.5cm]{university-logo}{logo_p7.jpg}
 \logo{\pgfuseimage{university-logo}}

\begin{document}

\begin{frame}
\maketitle
\end{frame}

\begin{frame}
\frametitle{Introduction : but de l'analyse syntaxique}
\pause
\textbf{Objectif} : mettre en \'evidence la structure d'un \'enonc\'e

\vspace{1cm}
\pause

\textbf{Buts} :
\pause

\begin{itemize}
\item Pr\'eparation \`a l'analyse s\'emantique
\pause
\item Test de grammaticalit\'e
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{\'Etapes d'analyse}
\pause

\textbf{Ex} : \texttt{200*6+137}

\pause
\textbf{Segmentation} : \texttt{[200, *, 6, +, 137]}

\pause
\textbf{Analyse syntaxique}

\Tree [ .E [ .E [ .T [ .T [ .F 200 ] ] * [ .F 6 ] ] ] + [ .T [ .F 137 ] ] ]

\end{frame}

\begin{frame}
\frametitle{Grammaires formelles}

\pause
\textbf{D\'efinition formelle}

$$G = \left< X, V, S, \delta \right>$$

\pause
\begin{itemize}
\item $X$ un ensemble fini de symboles terminaux
\pause
\item $V$ un ensemble fini de symboles non-terminaux
\pause
\item $S \in V$ l'axiome de la grammaire
\pause
\item $\delta$ un ensemble de r\`egles de r\'e\'ecriture
\end{itemize}

\pause
\textbf{Forme normale de Chomsky}
\pause
\begin{itemize}
\item R\`egles lexicales
\begin{itemize}
\item $NC \rightarrow cornemuse$
\end{itemize}
\pause
\item R\`egles non-lexicales
\begin{itemize}
\item $NP \rightarrow DET\;NC$
\item $NP \rightarrow NPP$
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{PCFG : grammaires probabilistes}
\pause
\textbf{D\'efinition formelle}

$$G = \left< X, V, S, \delta, \textcolor{red}{P} \right>$$

\pause
\textbf{Probabilit\'e d'une r\`egle}

$$P(X = A \rightarrow \alpha) = P(\alpha \mid A)$$

\pause
\textbf{Probabilit\'e d'un arbre de d\'erivation}

$$P(T = t) = \prod_{i=1}^{n} P(\alpha_i \rightarrow \beta_i)$$

\end{frame}

\begin{frame}
\frametitle{\'Etiquettage morpho-syntaxique (tagging)}
\pause
\textbf{Objectif} :

\begin{center}Assigner \`a chaque symbole terminal un symbole non-terminal\end{center}

\vspace{1cm}

\pause
\textbf{Strat\'egies} :
\begin{itemize}
\pause
\item Utiliser les r\`egles lexicales de la grammaire
\pause
\item Faire appel \`a un tagger externe
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Apprentissage de la grammaire : TrainSetReader}
Module TrainSetReader
\pause
\begin{itemize}
  \item \textbf{Entr\'ee} : corpus arbor\'e
  \pause
  \item \textbf{Sortie} : grammaire hors-contexte probabiliste non-CNF
\end{itemize}

\pause
Corpus utilis\'e : \textbf{Sequoia TreeBank}
 \begin{itemize}
  \pause
  \item $\approx$ 3200 phrases annot\'ees
  \pause
  \item arbres sous forme parenth\'es\'ee
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Apprentissage de la grammaire : TrainSetReader}
Transformer une phrase en un ensemble de r\`egles : 
\begin{itemize}
  \item chaque noeud d'un arbre est une production (sauf les noeuds terminaux)
  \item LHS = noeud de l'arbre, RHS = ses enfants
\end{itemize}
\pause
Calculer la probabilit\'e de chaque r\`egle : 
\begin{itemize}
  \item calcul par fr\'equences relatives
  \item LHS = cl\'e, RHS = valeur compt\'ee
  \item pour chaque RHS, $P(LHS \rightarrow RHS_{i}) = \frac{counts(LHS
  \rightarrow RHS_{i})}{\sum_{RHS}counts(LHS \rightarrow RHS)}$
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Parser CKY probabiliste}
\begin{itemize}
  \item - algorithme de parsing ascendant (Cocke-Kasami-Younger)
  \item - utilise une grammaire `invers\'ee'' : cl\'e = partie droite
  (RHS), valeur = toutes les productions avec cette RHS
  \item - multiplie les probabilit\'es \`a chaque niveau
  \item - peut int\'egrer la t\^ache du tagging : $$Phrase \rightarrow Tagger \rightarrow Parser (grammaire non-lexicale)$$ $$Phrase \rightarrow Parser (grammaire avec des productions lexicales)$$
  \item - donne l'analyse le plus probable
  \item - peut donner k-best analyses
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Parser CKY probabiliste}
Exemple d'ambiguit\'e du rattachement du syntagme pr\'epositionnel :\\
\textbf{la fille voit le professeur avec un t\'elescope} \\~\\

\only<1>{\textbf{Analyse 1} : $VP \rightarrow VP\:PP$ \\ \Tree [ .VP [
.VP [ .V voit ] [ .NP [ .DET le ] [ .N professeur ] ] ] [ .PP [ .P avec ] [ .NP [ .DET
un ] [ .N telescope ] ] ] ]}
\only<2>{\textbf{Analyse 2} : $NP \rightarrow NP\:PP$ \\
\Tree [ .VP [ .V voit ] [ .NP [ .NP [ .DET le ] [ .N professeur ] ] [ .PP [ .P
avec ] [ .NP [ .DET un ] [ .N telescope ] ] ] ] ]}
\only<3>{
\begin{columns}
\begin{column}{3cm}
\textbf{Grammaire (r\'egles lexicales omises) :} \\ $S
\rightarrow NP\:VP$ \\ $NP \rightarrow NP\:PP$ \\ $NP \rightarrow Det\:N$ \\ $VP \rightarrow
VP\:PP$ \\ $VP \rightarrow V\:NP$ \\ $VP \rightarrow V$ \\ $PP \rightarrow
P\:NP$
\end{column}
\begin{column}{7cm}
Analyse 1 : $VP \rightarrow VP\:PP$ (la fille a le t\'elescope) \\
$P(analyse_{1}) = P(VP \rightarrow VP\:PP) * P(VP \rightarrow V\:NP) * P(PP \rightarrow
P\:NP) * $ \ldots toutes les productions pr\'ec\'edentes  \\~\\

Analyse 2 : $NP \rightarrow NP\:PP$ (le professeur a le
telescope) \\
$P(analyse_{2}) = P(VP \rightarrow V\:NP) * P(NP \rightarrow NP\:PP) * P(PP
\rightarrow P\:NP) * $ \ldots toutes les productions pr\'ec\'edentes
\end{column}
\end{columns}
}
\end{frame}

\begin{frame}
\frametitle{Traitement des mots inconnus}
M\^eme le plus grand corpus ne contient pas tous les mots de la langue. Quoi
faire si on rencontre un mot inconnu?
\begin{itemize}
  \item - approche primitive : ignorer les mots inconnus, arr\^eter le parsing
  d\`es qu'on en voit un
  \item - approche plus sophistiqu\'ee : donner une probabilit\'e aux r\`egles
  $NONTERMINAL \rightarrow TERMINAL\_INCONNU$
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Traitement des mots inconnus}
\textbf{Strat\'egie 1 : probabilit\'e apriori} \\~\\

Choisir une probabilit\'e $\alpha = P(UNKNOWN|CAT)$. \\
Pour toute r\`egle lexicale, multiplier sa probabilit\'e par $(1 - \alpha)$,
pour avoir $\sum_{term} P(term|CAT) = 1$. \\~\\

Pour augmenter l'\'efficacit\'e du parseur :

\begin{itemize}
  \item choisir un param\'etre $\alpha$ diff\'erent pour chaque cat\'egorie :
  $P(UNKNOWN | NOM) > P(UNKNOWN | PREP)$
  \item faire varier le param\'etre $\alpha$
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Traitement des mots inconnus}
\textbf{Strat\'egie 2 : mots rares} \\~\\

Choisir une limite $t$. Si un mot $w$ est recontr\'e moins de $t$ fois dans le
corpus d'apprentissage, il est consid\'er\'e \textbf{rare}. On ajoute la
probabilit\'e de produire $w$ \`a la probabilit\'e de produire un mot inconnu. 
$$\sum_{rare} P(rare|CAT) = P(UNKNOWN|CAT)$$
=> on a la valeur de P(UNKNOWN|CAT) pour chaque cat\'egorie
s\'epar\'ement. \\~\\

Pour augmenter l'\'efficacit\'e du parseur :

\begin{itemize}
  \item utiliser un lemmatiseur en pr\'e-traitement, pour \'eviter de manquer
  des formes fl\'echies
  \item faire varier le param\'etre $t$ (la limite)
\end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{\'Evaluation de tagging}
\begin{block}{Tagging}
 \textbf{\`A \'evaluer}: assignation des cat\'egories morpho-syntaxiques aux tokens (mot-occurrences).
\end{block}
\pause
 (Pr\'ecision = Rappel) du taggueur:
$$ \frac{\# \text{cat\'egories bien identifi\'ees}}{\# \text{total de tokens dans la phrase}} $$
\pause
\begin{columns}[t]
\begin{column}{5cm}
\begin{block}{MElt (mod\`ele MaxEnt)}
92 \%
\end{block}
\end{column}
\begin{column}{5cm}
\begin{block}{CKY muni d'une grammaire lexicale}
88 \%
\end{block}
\end{column}
\end{columns}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{\'Evaluation de parsing}

\onslide<1->{
\begin{block}{Ce qu'on pourrait faire}
 Compter les arbres (issus du parseur) enti\`erement identiques \`a ceux du corpus arbor\'e (gold).
\end{block}
}
\onslide<2>{
\begin{block}{Pourquoi pas ?}
 \begin{itemize}
 \item Pas r\'ealistique % de s'attendre \`a ce que les arbres soient identiques
 \item Analyse pas assez fine
 \end{itemize}
\end{block}
}

\end{frame}

%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{\'Evaluation de parsing}

\only<1,3>{
\begin{align*}
\left[_{\color<3>{red} NP} \left[_{\color<3>{red} DET} \text{\tt le}_1 \right]
\left[_{\color<3>{red} NP}
\left[_{\color<3>{red} N} \text{\tt professeur}_2 \right]
\left[_{\color<3>{red} PP} \left[_{\color<3>{red} P} \text{\tt avec}_3 \right]
\left[_{\color<3>{red} NP} \left[_{\color<3>{red} DET} \text{\tt le}_4 \right] \left[_{\color<3>{red} N} \text{\tt telescope}_5 \right]  \right]
\right]
 \right]
\right]
\end{align*}
}
\only<2>{
\begin{align*}
\left[ \left[ \text{\tt le}_1 \right]
\left[
\left[ \text{\tt professeur}_2 \right]
\left[ \left[ \text{\tt avec}_3 \right]
\left[ \left[ \text{\tt le}_4 \right] \left[ \text{\tt telescope}_5 \right]  \right]
\right]
 \right]
 \right]
\end{align*}
}

\onslide<1->{
\begin{block}{Parsing}
 \textbf{\`A \'evaluer}:
 \begin{enumerate}
 \item {\color<2>{red} D\'ecoupage en constituants \textbf{(Bracketing measures)}}
 \item D\'ecoupage en constituants + {\color<3>{red} l'\'etiquettage de constituants (types de syntagmes) \textbf{(Labeled measures)}}
 \end{enumerate}
\end{block}
}

 \onslide<4->{
\begin{multline*}
\text{Rappel} = \frac{\# \text{constituants correctes} }{\#
\text{constituants gold} }
\end{multline*}
\begin{multline*}
\text{Precision} = \frac{\# \text{constituants correctes} }{\#
\text{constituants issus du parseur} }
\end{multline*}
\begin{multline*}
\text{F-score} = \frac{2 \text{Rappel} \text{Precision} }{ \text{Rappel} + \text{Precision} }
\end{multline*}
}




\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{\'Evaluation de parsing}
\pause
\begin{block}{}
 L'outil utilis\'e pour l'\'evaluation : \textbf{EVALB}.
\end{block}
\pause

\begin{block}{\'Evaluer le parseur seul (tagging gold)}
\begin{columns}[t]
\begin{column}{5cm}
\begin{block}{Bracketing}
Recall         =  64.19
\\
Precision      =  67.85
\\
F-score       =  65.97
\end{block}
\end{column}
\begin{column}{5cm}
\begin{block}{Labeled}
Recall         =  62.99
\\
Precision      =  66.58
\\
FMeasure       =  64.74
\end{block}
\end{column}
\end{columns}
\end{block}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{\'Evaluation de parsing}

\onslide<1->{
\begin{block}{}
 L'outil utilis\'e pour l'\'evaluation : \textbf{EVALB}.
\end{block}

\'Evaluer l'enchainement (tagging + parsing).
}

\onslide<2->{
\begin{block}{Avec le taggeur MElt}
Bracketing Recall         =  61.49
\\
Bracketing Precision      =  64.01
\\
Bracketing FMeasure       =  62.73
\end{block}
}

\onslide<3->{
\begin{block}{CKY tout seul}
\only<4>{
\begin{itemize}
\item \textbf{MODE: donner une probabilit\'e apriori d'un mot inconnu}
\\
Bracketing Recall         =  55.52
\\
Bracketing Precision      =  62.16
\\
Bracketing FMeasure       =  58.65
\end{itemize}
}
\only<5>{
\begin{itemize}
\item \textbf{MODE: consid\'erer les mots rares comme inconnus}
\\
Bracketing Recall         =  58.48
\\
Bracketing Precision      =  62.67
\\
Bracketing FMeasure       =  60.51
\end{itemize}
}
\end{block}
}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{\'Evaluation de parsing: conclusions}
\pause
\begin{enumerate}
\item Il ne faut pas laisser tout le travail au parseur, mais utiliser d'abord un taggueur
\pause
\item En g\'en\'eral les r\'esultats laissent \`a d\'esirer
\end{enumerate}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Sources d'inn\'efficacit\'es, am\'eliorations possibles}
\pause
\begin{enumerate}
\item \textbf{Taille du corpus}:
\\ $\approx$ 1000 phrases de Wikip\'edia en total,
\\ $\approx$ 600 phrases dans le corpus d'entrainement
\pause
\item \textbf{Param\`etres de gestion de mots inconnus}
\pause
\begin{itemize}
\item Int\'egration de l'information morphologique (la flexion)
\\ dans l'estimation des probabilit\'es de mots inconnus
\end{itemize}
\pause
\item \textbf{Limites des capacit\'es de CKY probabiliste}
\pause
\begin{itemize}
\item Parsing non-lexicalis\'e
\pause
\item Trop d'hypoth\`eses d'ind\'ependance:
\\ un noeud se re\'ecrit ind\'ependamment de ses fr\^eres et anc\^etres dans l'arbre
syntaxique
\end{itemize}
\end{enumerate}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Am\'eliorations possibles: extensions de CKY probabiliste}
\begin{enumerate}
\setcounter{enumi}{0}
\item \textbf{Algorithme standard}:
$$P(\beta \mid \text{type de syntagme})$$
\end{enumerate}

\pause
\begin{enumerate}
\item \textbf{Lexicalisation}:
$$P(\beta \mid \text{type de syntagme}, \text{t\^ete lexicale})$$
\pause
\item \textbf{Markovization horizontale}:
\pause
\\ Ajouter la d\'ependance des fr\^eres dans l'arbre
$$P(\beta \mid \text{type de syntagme}, \text{des noeuds du m\^eme niveau dans l'arbre})$$
\pause
\item \textbf{Markovization verticale}:
$$P(\beta \mid \text{type de syntagme}, \text{le parent du noeud})$$
\end{enumerate}

\pause
Probl\`emes ?
\pause
Donn\'ees \'eparses $\Rightarrow$ N\'ec\'essit\'e de techniques de lissage.

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Am\'eliorations possibles: question de la complexit\'e}

\begin{block}{Impl\'ementation actuelle}
\begin{itemize}
\item \textbf{Un seul meilleur arbre}:
\\
algorithme \`a la Viterbi
\item \textbf{k meilleurs arbres}:
\\ parcours de tous les arbres possibles dans chaque case pour chaque symb\^ole
\end{itemize}
\end{block}

\pause

Chercher \`a r\'eduire la compl\'exit\'e de la version $\mathrm{k\-best}$ !

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}