\documentclass[12pt]{article}
\usepackage[francais]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

\usepackage{amsmath,amssymb,enumerate,cite}
\usepackage{amsthm}
\usepackage{multimedia}
\usepackage[mathscr]{eucal}
\usepackage{srcltx}
\usepackage{enumitem}
\usepackage{graphicx,enumerate}
\usepackage{gb4e}

\begin{document}

\section{Pr\'esentation du probl\`eme}

Le pr\'esent projet s'inscrit dans le cadre du domaine de Traitement Automatique
des Langues Naturelles. L'enjeu principal de ce domaine est d'apprendre aux ordinateurs  \`a extraire de l'information des textes en langage naturel en minimisant l'implication des humains. G\'en\'eralement l'analyse automatique de ces textes se fait par plusieurs \'etapes selon leurs niveaux de sophistication:
\begin{enumerate}
\item D\'ecoupage du texte en phrases.
\item (Tok\'enisation) D\'ecoupage du texte en mot-occurrences (tokens).
\item (Tagging) Identification de la cat\'egorie morpho-syntaxique de chaque mot-occurrence.
\item (Parsing) Analyse syntaxique.
\item Analyse s\'emantique.
\end{enumerate}

Dans notre projet nous nous concentrons sur l'analyse syntaxique. Cependant, d\`es lors que pour l'\'evaluation de notre outil on souhaite se rapprocher des situations r\'eelles et donc travailler avec des textes bruts, on passe in\'evitablement par les trois premi\`eres \'etapes aussi. La derni\`ere \'etape list\'ee ci-dessus ne fait en revanche pas partie de notre travail...

\subsection{Analyse syntaxique en constituants}
 On suppose qu'en entr\'ee on a une s\'equence de mots provenant d'un certain langage $L_0$. Le but est de trouver l'arbre syntaxique de la phrase repr\'esent\'ee par cette s\'equence dans une grammaire donn\'ee.

\subsection{Le but (l'utilit\'e) de l'analyse syntaxique}

Il faut avouer que l'analyse syntaxique en soi est rarement le but d'un outil de
TAL quelconque. Par contre, c'est une \'etape qui s'av\`ere cruciale pour la
possibilit\'e d'une analyse s\'emantique effective, car toute description
s\'emantique se base en partie sur les relations entre les parties d'une
phrase.

\subsection{La grammaire}

D\'efinition d'une grammaire formelle.

\subsection{ambig\"uit\'e syntaxique et grammaires probabilistes}

Une des propri\'et\'es les plus distinctives des langages naturels qui les
oppose aux langages de programmation, par exemple, est leur ambig\"uit\'e inh\'erente. L'ambig\"uit\'e peut se pr\'esenter sur des niveaux d'analyse diff\'erents:
\begin{enumerate}
\item ambig\"uit\'e de d\'ecoupage en mot-formes:
//
EXEMPLE ... d'accord d'+accord ...
\item ambig\"uit\'e morpho-syntaxique:
//
EXEMPLE ... la belle porte le voile ...
\item ambig\"uit\'e syntaxique:
EXEMPLE ... la description de l'occupation d'Ath\`enes par les Turcs ottomans ... 
\end{enumerate}

Il est important de distinguer deux types d'ambig\"uit\'e, le
premier \'etant l'ambig\"uit\'e r\'eelle qui correspond au cas quand l'ambig\"uit\'e
ne peut pas \^etre enlev\'ee par des humains eux-m\^emes en absence de contexte,
et le deuxi\`eme \'etant l'ambig\"uit\'e artificielle qui ne se pr\'esente que pour des machines --- pour les humains il n'existe dans ces cas de figure qu'une seule variante d'analyse possible.

On note ici que m\^eme si les deux types d'ambig\"uit\'e sont pr\'esents sur tous les niveaux d'analyse, l'ambig\"uit\'e r\'eelle est relativement rare en tok\'enisation et tagging, tandis que l'analyse syntaxique
rel\`eve largement d'ambig\"uit\'e r\'eelle aussi bien qu'artificielle. Parmi les cas les plus r\'epandus de l'ambig\"uit\'e syntaxique r\'eelle on peut mentionner l'ambig\"uit\'e de rattachement pr\'epositionnel : 

\begin{exe}
\ex La fille voit le professeur avec un t\'elescope.
\end{exe}

Et l'ambig\"uit\'e de structures de coordination :

\begin{exe}
\ex Elle ne mange que des gateaux et des glaces au chocolat.
\end{exe}

Ce constat nous am\`ene \`a conclure que l'un des piliers d'une
analyse syntaxique est le traitement de l'ambig\"uit\'e. Deux solutions sont
envisageables vis \`a vis de ce probl\`eme. On pourrait choisir de ne garder
qu'une seule analyse par phrase, ce qui semble \^etre peu satisfaisant \'etant
donn\'ee l'abondance d'ambig\"uit\'e r\'eelle susmentionn\'ee. L'option de
g\'en\'erer plusieurs arbres syntaxiques pour chaque phrase se pr\'esente comme
plus avantageuse dans le cadre de ce travail. Dans les cas d'ambig\"uit\'e
r\'eelle, c'est l'humain qui choisit le bon analyse en se basant sur le
contexte.

\begin{exe}
\ex Fish people fish tanks
\end{exe}

Une grammaire probabiliste hors contexte (Probabilistic context-free grammar,
PCFG) est une grammaire hors contexte dans laquelle une probabilit\'e est
attribu\'ee \`a chaque r\`egle. La somme des toutes les probabilit\'es des
productions \`a partir d'une m\^eme partie gauche est \'egale \`a un. 
En cas d'ambig\"uit\'e, les probabilit\'es associ\'ees aux productions permettent
de choisir la d\'erivation la plus probable de la phrase. On appelle la
probabilit\'e d'une d\'erivation le produit des probabilit\'es des r\`egles de
production qui ont men\'e \`a cette d\'erivation. Il faut noter que la
probabilit\'e maximale de la d\'erivation de la phrase n'est pas toujours obtenue
en appliquant les r\`egles avec les probabilit\'es maximales.


\section{Choix de m\'ethodologie et d'impl\'ementation}

\subsection{Deux strat\'egies d'analyse}

Supposons que dans notre grammaire $G = \left< \Sigma, \mathscr{V}, S, \mathscr{R}, \tilde{\mathrm{P}} \right>$ l'ensemble de r\`egles est l'union disjointe $\mathscr{R} = \mathscr{R}_{LEX} \uplus \mathscr{R}_{NL}$ de deux types de r\`egles:
\begin{itemize}
\item des r\`egles lexicales
$$\mathscr{R}_{LEX} = \left\{ A \rightarrow a \mid
        A \in \mathscr{V}, \, a \in \Sigma \right\}$$
qui sont des appariements de mot-formes et de leurs cat\'egories syntaxiques
\item des r\`egles non-lexicales
$$\mathscr{R}_{NL} = \left\{ A \rightarrow \beta \mid
     A \in \mathscr{V}, \beta \in \mathscr{V}^* \right\},$$
qui ne contiennent pas de symboles terminaux.
\end{itemize}

Cette supposition peut \^etre faite sans perte de g\'en\'eralit\'e puisque
chaque grammaire hors-contexte peut \^etre transform\'ee en forme susdite sans
que son langage engendr\'e ne soit chang\'e. En m\^eme temps, en adoptant cette
approche, on peut envisager deux strat\'egies de conception de notre outil de
l'analyse syntaxique.

Deux strat\'egies.

\subsection{Corpus d'apprentissage, de d\'eveloppement et de test}

Sequoia Tree Bank. \cite{Sequoia}

\subsection{Apprentissage de la grammaire probabiliste}

En g\'en\'eral, l'estimation des probabilit\'es des r\`egles d'une grammaire
hors-contexte probabiliste (PCFG) se fait par fr\'equences relatives:
\begin{eqnarray}
P(A \rightarrow \beta) = \frac{ct(A \rightarrow \beta)}{\sum\limits_{\gamma \in (\Sigma \cup \mathcal{V})^*}{ct(A \rightarrow \gamma)}},
\end{eqnarray}
$ct(A \rightarrow \beta)$ --- le nombre de fois que le non-terminal $A$ se r\'eecrit en $\beta$ dans le corpus d'apprentissage.

Il est important de noter ici que le probl\`eme d'estimation des probabilit\'es
d'un PCFG vient du fait que le corpus d'entra\^inement, aussi grand soit-il,
n'est jamais complet, donc il y a toujours des mots-formes qui ne sont pas
pr\'esents dans la grammaire et dont la cat\'egorie lexicale et la
probabilit\'e d'emission sont donc inconnues. Pour d\'eterminer la cat\'egorie
lexicale il est possible d'avoir recours \`a un dictionnaire externe. Mais il
n'existe pas de solution por conna\^itre la probabilit\'e de la production de ce
mot inconnu, donc des m\'ethodes artificielles sont utilis\'ees pour estimer
cette probabilit\'e.

Dans leur article de 1998 Chi et Geman ont prouv\'e qu'une PCFG estim\'ee par
fr\'equences relatives sur un corpus annot\'e est toujours consistente \cite{proper_PCFG_estimation}.

\subsection{Tok\'enisation et analyse morpho-syntaxique}

L'utilisation de l'outil MELt.

\subsection{Le parseur tabulaire de Cocke, Younger et Kasami (CKY)}

\subsubsection{L'algorithme standard}
\subsubsection{L'injection des probabilit\'es dans l'algorithme}

\subsection{La transformation de la grammaire probabiliste en forme normale de Chomsky flexible}

\subsection{Le traitement des mots inconnus}

Si on adopte la strat\'egie 1 de l'analyse syntaxique, alors le traitement des mots inconnus est laiss\'e au taggeur.
Le taggeur MELt r\'esout ce probl\`eme avec succes (??). Faut-il \'ecrire qch sur comment est-ce que MELt le fait ?

On va se concentrer sur la r\'esolution de ce probl\`eme dans le cas de la strat\'egie 2 quand c'est le parseur CKY probabiliste qui assigne les cat\'egories morphosyntaxiques aux entr\'ees lexicales. \'Evidement si on estime les probabilit\'es des r\`egles lexicales sur le corpus d'entrainement, alors n'importe quelle phrase qui contient au moins un mot non-observ\'e dans ce corpus va recevoir une probabilit\'e nulle avec chaque arbre syntaxique de cette phrase \'etant aussi de probabilit\'e nulle. Par cons\'equent, selon le choix d'impl\'ementation soit le parseur probabiliste s'\'echouera, soit il envoyera un arbre syntaxique arbitraire (choisi parmi les arbres \'equiprobables). Pour contourner cet effet ind\'esirable il nous faut modifier notre proc\'edure d'estimation de la PCFG de fa\'con \`a ce que la probabilit\'e d'un mot inconnu soit non-nulle. Pour le faire on a choisi de tester deux m\'ethodes diff\'erentes.

\subsubsection{Le traitement des mots inconnus: m\'ethode 1}

Mod\'elisons la probabilit\'e d'un mot $w$ sachant sa cat\'egorie morphosyntaxique $C$ comme la probabilit\'e conjointe suivante
\begin{eqnarray}
\label{prob_mot_inconnu_0}
 P(W = w \mid \tilde{C} = C) = P(W = w, \mathrm{UNKNOWN} = unknown \mid \tilde{C} = C).
\end{eqnarray}
La valeur $unknown$ est \'egale \`a 1 si le mot $w$ a \'et\'e observ\'e dans le corpus d'entrainement et 0 sinon. D\`es lors que la valeur de la variable $\mathrm{UNKNOWN}$ est en d\'ependance directe de la valeur $w$,
on a affaire \`a une relation d'inclusion entre les \'ev\'enements: l'\'ev\'enement $W = w$ est inclu dans l'\'ev\'enement $\mathrm{UNKNOWN} = unknown$, donc l'\'egalit\'e \ref{prob_mot_inconnu_0} tient grace \`a l'axiome de th\'eorie des probabilit\'es suivante:
$$ A \subset B \Rightarrow P(A) = P(A,B).$$

Adoptons que pour chaque cat\'egorie morphosyntaxique $C$ la probabilit\'e de rencontrer un mot inconnu est toujours la m\^eme et est \'egale \`a $\alpha$:
$$ \forall C \text{--- cat\'egorie morphosyntaxique} \; P(\mathrm{UNKNOWN} = 1 \mid \tilde{C} = C) = \alpha.$$

Ensuite on va d\'ecomposer la probabilit\'e en \ref{prob_mot_inconnu_0} en utilisant la d\'efinition de la probabilit\'e conditionnelle.
\begin{multline}
\label{prob_mot_inconnu_cond}
 P(W = w \mid \tilde{C} = C) = P(W = w, \mathrm{UNKNOWN} = unknown \mid \tilde{C} = C) = 
 \\
 P(\mathrm{UNKNOWN} = unknown \mid \tilde{C} = C) P(W = w \mid \mathrm{UNKNOWN} = unknown , \tilde{C} = C).
\end{multline}

 Si le mot $w$ a \'et\'e observ\'e dans le corpus d'entrainement, alors on a
\begin{multline}
\label{prob_mot_connu_1}
 P(W = w \mid \tilde{C} = C) = P(W = w, \mathrm{UNKNOWN} = 0 \mid \tilde{C} = C) = 
 \\
 P(\mathrm{UNKNOWN} = 0 \mid \tilde{C} = C) P(W = w \mid \mathrm{UNKNOWN} = 0, \tilde{C} = C) =
 \\
 (1 - \alpha) P(W = w \mid \mathrm{UNKNOWN} = 0, \tilde{C} = C).
\end{multline}
La probabilit\'e $P(W = w \mid \mathrm{UNKNOWN} = 0, \tilde{C} = C)$ est estim\'e 
comme d'habitude par la fr\'equence relative de la paire $(w, C)$ dans le corpus d'entrainement:
\begin{eqnarray*}
\hat{P}(W = w \mid \mathrm{UNKNOWN} = 0, \tilde{C} = C) =
\frac{ct(C \rightarrow w)}{\sum\limits_{a \in \Sigma}{ct(C \rightarrow a)}}.
\end{eqnarray*}

Ensuite on va traiter tous les mots inconnus comme un seul mot en ne faisant pas distinction entre eux. 
Par cons\'equent on peut supposer qu'on a pretrait\'e notre corpus de developpement (ou de test) de fa\'con \`a remplacer tous les mots inconnus par le m\^eme mot $w_{unknown}$ (qui n'a jamais \'et\'e rencontr\'e dans le corpus d'entrainement). Alors 
$$\forall C \; P(W = w_{unknown} \mid \mathrm{UNKNOWN} = 1, \tilde{C} = C) = 1$$
et la probabilit\'e d'un mot inconnu est calcul\'ee comme suit
\begin{multline}
\label{prob_mot_inconnu_1}
 P(W = w_{unknown} \mid \tilde{C} = C) = P(W = w_{unknown}, \mathrm{UNKNOWN} = 1 \mid \tilde{C} = C) =
 \\
 P(\mathrm{UNKNOWN} = 1 \mid \tilde{C} = C) P(W = w_{unknown} \mid \mathrm{UNKNOWN} = 1, \tilde{C} = C) =
 \\
 P(\mathrm{UNKNOWN} = 1 \mid \tilde{C} = C) = \alpha.
\end{multline}


\subsection{\'Evaluation}

Etapes \`a \'evaluer :
\begin{itemize}
\item Segmentation de l'entr\'e (Tok\'enisation)
\item \'Etiqu\'etage morpho-syntaxique (Tagging)
\item Analyse syntaxique (Parsing)
\end{itemize}

Types d'entr\'ee :
\begin{itemize}
\item Texte non-annot\'e et non-segment\'e (en mots)
\item Texte segment\'e (tok\'enis\'e)
\item Texte morpho-syntaxiquement \'etiquet\'e (tagg\'e)
\item Corpus arbor\'e (muni d'annotations syntaxiques)
\end{itemize}


\section{R\'esultats}

\section{Utilisation}
\subsection{TrainSetReader}
\subsubsection{Nom}
 
TrainSetReader --- Générer une grammaire à partir d'un corpus d'apprentissage.
 
\subsubsection{Synopsis}
 
{\ttfamily
\begin{verbatim}
java -jar TrainSetReader.jar [-p precision] [-t nthreads] [-u
unknown_threshold] [-s unknown_label] [-l] [files...]
\end{verbatim}
}
 
\subsubsection{Description}
 
Le programme \texttt{TrainSetReader} lit un texte annoté en constituants
syntaxiques sous forme arborée, selon les conventions utilisées par le Sequoia Tree Bank. Il en
génère ensuite une grammaire algébrique probabiliste, chaque règle de réécriture
voyant sa probabilité estimée par un calcul de fréquence relative. Cette
grammaire est affichée sur la sortie standard.
 
Si des fichiers d'entrée sont spécifiés, le programme lira seulement ceux-ci. Si
aucun fichier d'entrée n'est spécifié, les corpora d'entraînement seront lus sur
l'entrée standard.

\subsubsection{Options}

\begin{description}[style=nextline]
\item[\texttt{-p, --precision precision}] Spécifie un nombre de chiffres après
la virgule qui seront affichés avec chaque règle de réécriture. Sa valeur par défaut est
\texttt{10}.
\item[\texttt{-t, --nthreads threads}] Spéficie le nombre de fils d'exécution
qui seront exécutés en même temps par le programme.
\item[\texttt{-u, --unknown-threshold unknown\_threshold}] Spécifie le nombre
d'occurrences dans le corpus en-dessous duquel la partie droite d'une règle de
réécriture sera considérée comme rare, et donc inconnue.
\item[\texttt{-s, --unknown-label unknown\_label}] Spécifie le label qui sera
attribué aux symboles inconnus.
\item[\texttt{-l, --lexical}] Si cette option est spécifiée, le programme
affichera la grammaire avec ses règles lexicales (dont la partie droite est un
symbole terminal). Celles-ci ne sont pas affichées par défaut.
\end{description}


\begin{thebibliography}{9}
    \bibitem{Jurafsky}
    Jurafsky,~D., Martin,~J., 2009. \emph{Speech and Language Processing: An Introduction to Natural Language Processing, Speech Recognition, and Computational Linguistics.} 2nd edition. Prentice-Hall.
    \bibitem{Sequoia}
    Candito,~M., Seddah,~D.
    \emph{Le corpus Sequoia: annotation syntaxique et exploitation pour
    l'adaptation d'analyseur par pont lexical.} Actes de TALN'2012, Grenoble,
    France.
    \bibitem{proper_PCFG_estimation}
    Chi,~Z., Geman,~S., 1998.
    \emph{Estimation of probabilistic context-free grammars.}, Computational Linguistics, 24(2):299-305.
    \bibitem{Jurafsky-Manning}
    Jurafsky,~D., Manning,~C., 2012. \emph{Free online course on Natural
    Language Programming.}
    \bibitem{MElt tagger} 
    Denis,~P., Sagot,~B.,2012.
    \emph{Coupling an annotated corpus and a lexicon for state-of-the-art POS
    tagging.} In Language Resources and Evaluation 46:4, pp. 721-736,
    DOI 10.1007/s10579-012-9193-0.
    
\end{thebibliography}

\end{document}


