\documentclass[12pt]{article}
\usepackage[francais]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{parskip}
\usepackage{amsmath,amssymb,enumerate,cite}
\usepackage{amsthm}
\usepackage{multimedia}
\usepackage[mathscr]{eucal}
\usepackage{srcltx}
\usepackage{enumitem}
\usepackage{graphicx,enumerate}
\usepackage{gb4e}
\noautomath

\begin{document}

\title{Analyse syntaxique en constituants}
\author{Yulia MATVEEVA, Antoine LAFOUASSE, Kira KIRANOVA\\
M1 Linguistique Informatique\\
Universit\'e Paris 7 - Paris Diderot}
\date{}
\maketitle

\section{Pr\'esentation du probl\`eme}

Le pr\'esent projet s'inscrit dans le cadre du domaine de Traitement Automatique
des Langues Naturelles. L'enjeu principal de ce domaine est d'apprendre aux ordinateurs  \`a extraire de l'information des textes en langage naturel en minimisant l'implication des humains. G\'en\'eralement l'analyse automatique de ces textes se fait par plusieurs \'etapes selon leurs niveaux de sophistication:

\begin{enumerate}
\item D\'ecoupage du texte en phrases.
\item (Tok\'enisation) D\'ecoupage du texte en mot-occurrences (tokens).
\item (Tagging) Identification de la cat\'egorie morpho-syntaxique de chaque
mot-occurrence.
\item (Parsing) Analyse syntaxique.
\item Analyse s\'emantique.
\end{enumerate}

Dans notre projet nous nous concentrons sur l'analyse syntaxique. Cependant, d\`es lors que pour l'\'evaluation de notre outil on souhaite se rapprocher des situations r\'eelles et donc travailler avec des textes bruts, on passe in\'evitablement par les trois premi\`eres \'etapes aussi. La derni\`ere \'etape list\'ee ci-dessus ne fait en revanche pas partie de notre travail.

\subsection{Analyse syntaxique en constituants}
 On suppose qu'en entr\'ee on a une s\'equence de mots provenant d'un certain
 langage $L_0$. Le but est de trouver l'arbre syntaxique de la phrase
 repr\'esent\'ee par cette s\'equence dans une grammaire donn\'ee.

\subsection{Le but (l'utilit\'e) de l'analyse syntaxique}

Il faut avouer que l'analyse syntaxique en soi est rarement le but d'un outil de
TAL quelconque. Par contre, c'est une \'etape qui s'av\`ere cruciale pour la
possibilit\'e d'une analyse s\'emantique effective, car toute description
s\'emantique se base en partie sur les relations entre les parties d'une
phrase. 

\subsection{La grammaire}

Une grammaire formelle hors contexte (Context-Free Grammar, CFG) est un
quadruplet $\langle \Sigma, V, S, R \rangle$, o\`u :
\begin{itemize}
\item $\Sigma$ est un ensemble fini des symboles terminaux (l'alphabet)
\item V est un ensemble fini des symboles non-terminaux
\item S est l'axiome (le symbole de d\'epart de la grammaire), $S \in V$
\item R est un ensemble fini des r\`egles de production de la forme $A
\rightarrow \alpha$, o\`u $A \in V$ et $\alpha \in (\Sigma \cup V)$
\end{itemize}

Chaque grammaire formelle correspond \`a un langage formel (un ensemble de mots).
Cela veut dire que tous les mots du langage peuvent \^etre engendr\'es par cette
grammaire. Les mots du langage sont constitu\'es des symboles de l'alphabet
$\Sigma$.\par

Dans notre projet, les \'el\'ements de l'alphabet sont des mots francais, donc les
mots du langage sont des phrases en francais. Un \'etape important du projet est de construire
une grammaire formelle \`a partir des donn\'ees linguistiques (un corpus des phrases
d\'ej\`a pars\'ees et annot\'ees) afin de pouvoir utiliser cette grammaire pour trouver
la structure syntaxique de n'importe quelle phrase en francais.

\subsection{Ambig\"uit\'e syntaxique et grammaires probabilistes}

Une des propri\'et\'es les plus distinctives des langages naturels qui les
oppose aux langages de programmation, par exemple, est leur ambig\"uit\'e inh\'erente. L'ambig\"uit\'e peut se pr\'esenter sur des niveaux d'analyse diff\'erents:

\begin{enumerate}
\item Ambig\"uit\'e de d\'ecoupage en mot-formes :
\begin{exe}
\ex d'accord - d'+accord
\end{exe}
\item Ambig\"uit\'e morpho-syntaxique :
\begin{exe}
\ex la belle porte le voile
\end{exe}
\item Ambig\"uit\'e syntaxique :
\begin{exe}
\ex la description de l'occupation d'Ath\`enes par les Turcs ottomans
\end{exe}
\end{enumerate}


En parlant de l'ambig\"uit\'e, il est important d'en distinguer deux types :
\begin{itemize}
  \item l'ambig\"uit\'e \emph{r\'eelle}, qui correspond au cas quand l'ambig\"uit\'e
ne peut pas \^etre enlev\'ee par des humains eux-m\^emes en absence de contexte
\item l'ambig\"uit\'e \emph{artificielle}, qui ne se pr\'esente que pour des machines,
tandis que pour les humains il n'existe qu'une seule variante d'analyse possible
\end{itemize}

On note ici que m\^eme si les deux types d'ambig\"uit\'e sont pr\'esents sur tous les niveaux d'analyse, l'ambig\"uit\'e r\'eelle est relativement rare en tok\'enisation et tagging, tandis que l'analyse syntaxique
rel\`eve largement d'ambig\"uit\'e r\'eelle aussi bien qu'artificielle. Parmi les cas les plus r\'epandus de l'ambig\"uit\'e syntaxique r\'eelle on peut mentionner l'ambig\"uit\'e de rattachement pr\'epositionnel :

\begin{exe}
\ex La fille voit le professeur avec un t\'elescope.
\end{exe}

Et l'ambig\"uit\'e de structures de coordination :

\begin{exe}
\ex Elle ne mange que des gateaux et des glaces au chocolat.
\end{exe}

Ce constat nous am\`ene \`a conclure que l'un des piliers d'une
analyse syntaxique est le traitement de l'ambig\"uit\'e. Deux solutions sont
envisageables vis \`a vis de ce probl\`eme.\par

On pourrait choisir de ne garder qu'une seule analyse par phrase, ce qui semble \^etre peu satisfaisant \'etant
donn\'e l'abondance d'ambig\"uit\'e r\'eelle susmentionn\'ee (dans cette m\^eme phrase, le
lecteur peut remarquer la pr\'esence d'ambiguit\'e r\'eelle li\'ee au rattachement du
participe \emph{susmentionn\'ee}). \par

L'option de g\'en\'erer plusieurs arbres syntaxiques pour chaque phrase se pr\'esente comme
plus avantageuse dans le cadre de ce travail. Dans les cas d'ambig\"uit\'e
r\'eelle, c'est l'humain qui choisit la bonne analyse en se basant sur le
contexte. \par

Un petit exemple (en anglais) qui demontre le large
choix d'analyses pour une phrase ambigue et la n\'ecessit\'e de pouvoir garder plusieures analyses par phrase :

\begin{exe}
\ex fish people fish tanks
\end{exe}

Une grammaire probabiliste hors contexte (Probabilistic context-free grammar,
PCFG) est une grammaire hors contexte dans laquelle une probabilit\'e est
attribu\'ee \`a chaque r\`egle. La somme des toutes les probabilit\'es des
productions \`a partir d'une m\^eme partie gauche est \'egale \`a un. Par cons\'equent, 
la probabilit\'e associ\'ee \`a une r\`egle de la forme $A \rightarrow \beta$ peut \^etre interpr\'et\'e
comme la probabilit\'e conditionnelle $P(\beta \mid A)$ d'obtenir $\beta$ comme fils imm\'ediat de $A$ \'etant
donn\'e le symb\^ole $A$. \par

En cas d'ambig\"uit\'e, les probabilit\'es associ\'ees aux productions permettent
de choisir la d\'erivation la plus probable de la phrase. On appelle \textbf{la
probabilit\'e d'une d\'erivation} le produit des probabilit\'es des r\`egles de
production qui ont "particip\'e" \`a cette d\'erivation. Il faut noter que la
probabilit\'e maximale de la d\'erivation de la phrase n'est pas toujours obtenue
en appliquant \`a chaque \'etape de la d\'erivation les r\`egles avec les probabilit\'es
maximales. 

\section{Choix de m\'ethodologie et d'impl\'ementation}

\subsection{Deux strat\'egies d'analyse}

Supposons que dans notre grammaire $G = \left< \Sigma, \mathscr{V}, S, \mathscr{R}, \tilde{\mathrm{P}} \right>$ l'ensemble de r\`egles est l'union disjointe $\mathscr{R} = \mathscr{R}_{LEX} \uplus \mathscr{R}_{NL}$ de deux types de r\`egles:

\begin{itemize}
\item des r\`egles lexicales
$$\mathscr{R}_{LEX} = \left\{ A \rightarrow a \mid
        A \in \mathscr{V}, \, a \in \Sigma \right\}$$
qui sont des appariements de mot-formes et de leurs cat\'egories syntaxiques
\item des r\`egles non-lexicales
$$\mathscr{R}_{NL} = \left\{ A \rightarrow \beta \mid
     A \in \mathscr{V}, \beta \in \mathscr{V}^* \right\},$$
qui ne contiennent pas de symboles terminaux
\end{itemize}

Cette supposition peut \^etre faite sans perte de g\'en\'eralit\'e puisque
chaque grammaire hors-contexte peut \^etre transform\'ee en forme susdite sans
que son langage engendr\'e ne soit chang\'e. En m\^eme temps, en adoptant cette
approche, on peut envisager deux strat\'egies de conception de notre outil de
l'analyse syntaxique.

Les deux strat\'egies propos\'ees dans le projet sont les suivantes :
\begin{enumerate}
  \item L'\'etiqu\'etage morpho-syntaxique (tagging) et l'analyse syntaxique
  (parsing) sont \'effectu\'es par le m\^eme module (le parseur). Le
  parseur recoit en entr\'ee une phrase qui consiste des mots (des symboles terminaux (lexicaux)).
  Dans le cadre de cette strat\'egie, les r\`egles de production lexicales sont
  incluses dans la grammaire utilis\'ee par le parseur.
  \item L'\'etiquetage morpho-syntaxique est \'eff\'ectu\'e par un module s\'epar\'e du
  parseur, par exemple, par un logiciel externe. Le parseur re\'coit en entr\'ee la
  phrase \'etiqu\'et\'ee priv\'ee des symboles terminaux (mots), donc une s\'equence de
  cat\'egories morpho-syntaxiques. Dans le cadre de cette deuxi\`eme strat\'egie, le
  parseur n'a pas besoin d'avoir des r\`egles lexicales dans la grammaire sur
  laquelle il se base.
\end{enumerate}

\subsection{Corpus d'apprentissage, de d\'eveloppement et de test}
Pour pouvoir construire une grammaire assez repr\'esentative pour le parseur, il
faut apprendre cette grammaire sur un corpus avec des phrases d\'ej\`a
\'etiquet\'ees et analys\'ees syntaxiquement (corpus arbor\'e, tree bank). Pour notre projet, nous
avons choisi d'entra\^iner et d'\'evaluer le parseur sur le corpus arbor\'e de la
langue francaise Sequoia Tree Bank \cite{Sequoia}. Ce corpus contient des
phrases en fran\'caises, chacune d'elles \'etant munie d'une structure syntaxique. 
Pour chaque phrase il n'y a qu'une seule analyse en constituants propos\'ee.\par

Pour pouvoir \'evaluer la qualit\'e de l'extraction de la grammaire et la qualit\'e du
parsing, nous divisons le corpus en trois parties :
\begin{itemize}
  \item le corpus d'apprentissage (training set), qui est utilis\'e pour analyser
  les arbres syntaxiques des phrases du corpus et en construire des r\`egles de la
  grammaire;
  \item le corpus de d\'eveloppement (development set), qui est utilis\'e pour
  essayer de lancer le parseur avec des diff\'erentes options et comparer les
  r\'esultats;
  \item le corpus de test (test set), qui est utilis\'e pour l'\'evaluation :
  %% l'outil ne se r\'eduit pas au parseur
  il faut lancer le l'outil de l'analyse syntaxique, comparer les arbres obtenus avec les "vrais" arbres
  pour les m\^emes phrases dans le corpus (les analyses gold, gold parses) et
  calculer la pr\'ecision et le rappel de l'analyse. 
  L'\'etape de l'\'evaluation est abord\'e plus en d\'etail dans la section homonyme.
\end{itemize}

Afin d'avoir la grammaire la plus compl\`ete possible, nous avons fait le choix
d'utiliser la plus grande partie du corpus (60\% de toutes les phrases du
corpus) pour l'apprentissage. Le reste est divis\'e \'egalement entre les deux
autres parties, donc le corpus de d\'eveloppement et le corpus de test consistent
chacun de 20\% de la totalit\'e des phrases. \par

Pour \^etre s\^urs que les r\'esultats de l'\'evaluation ne d\'ependent pas de la mani\`ere
dont le corpus est divis\'e en parties, nous choisissons \`a chaque fois au hasard
la repartition des phrases entre les trois corpus (en gardant les proportions mentionn\'ees
ci-dessus).


\subsection{Apprentissage de la grammaire probabiliste}

En g\'en\'eral, l'estimation des probabilit\'es des r\`egles d'une grammaire
hors-contexte probabiliste (PCFG) se fait par fr\'equences relatives:
\begin{eqnarray}
P(A \rightarrow \beta) = \frac{ct(A \rightarrow \beta)}{\sum\limits_{\gamma \in (\Sigma \cup \mathcal{V})^*}{ct(A \rightarrow \gamma)}},
\end{eqnarray}
$ct(A \rightarrow \beta)$ --- le nombre de fois que le non-terminal $A$ se
r\'e\'ecrit en $\beta$ dans le corpus d'apprentissage.

Une telle estimation par fr\'equences relatives donne l'estimation par maximum de vraisemblence
sous l'hypoth\`ese d'ind\'ependance suivante. On suppose que dans un arbre syntaxique chaque 
symb\^ole se re\'ecrit ind\'ependamment de ses fr\`eres dans l'arbre (c'est \`a dire des noeuds qui se situe
au m\^eme niveau) aussi bien que de tous ces anc\^etres (des noeuds des niveaux plus hauts, donc plus proche de l'axiome). C'est avec ses hypoth\`eses d'ind\`ependance qu'on obtient la formule du calcul de la probabilit\'e
d'un arbre par produit des probabilit\'es des r\`egles de re\'ecriture qu'il contient. 

En outre, dans leur article de 1998 Chi et Geman ont prouv\'e qu'une PCFG estim\'ee par
fr\'equences relatives sur un corpus annot\'e est toujours propre
\cite{proper_PCFG_estimation}. Cela veut dire qu'aucune probabilit\'e positive
n'est assign\'ee aux arbres infinis.

Il est important de noter ici que le probl\`eme d'estimation des probabilit\'es
d'une PCFG vient du fait que le corpus d'entra\^inement, aussi grand soit-il,
n'est jamais complet, donc il y a toujours des mots-formes qui ne sont pas
pr\'esents dans la grammaire et dont la cat\'egorie lexicale et la
probabilit\'e d'\'emission sont donc inconnues. Pour d\'eterminer la cat\'egorie
lexicale il est possible d'avoir recours \`a un dictionnaire externe. Mais il
n'existe pas de solution por conna\^itre la probabilit\'e de la production de ce
mot inconnu, donc des m\'ethodes artificielles sont utilis\'ees pour estimer
cette probabilit\'e.


\subsection{Tok\'enisation et analyse morpho-syntaxique}

Comme mentionn\'e ci-dessus, il est possible d'utiliser deux strat\'egies
diff\'erentes pour l'\'etiquetage morpho-syntaxique : soit utiliser le parseur
lui-m\^eme, soit utiliser un logiciel externe. Dans ce deuxi\`eme cas, le taggeur
(\'etiqueteur) MElt \cite{MElt} peut \^etre utilis\'e pour \'etiqueter (eventuellement
en tok\'enisant d'abord) la phrase donn\'ee. Le principe de l'\'etiqueteur MElt est
bas\'e sur les mod\`eles de Markov (Maximum-Entropy Markov Models).
Nous pouvons alors comparer l'\'efficacit\'e des deux strat\'egies de tagging.

\subsection{Le parseur tabulaire de Cocke, Younger et Kasami (CKY)}

L'algorithme utilis\'e pour le parseur dans notre projet est l'algorithme de
Cocke, Younger et Kasami (CKY, parfois not\'e CYK). C'est un algorithme d'analyse
ascendante (bottom-up parsing), c'est-\`a-dire qu'il part de la phrase tok\'enis\'ee
et remonte vers l'axiome de la grammaire pour recr\'eer en ordre inverse toutes les r\`egles de la grammaire qu'il
aurait fallu appliquer pour engendrer cette phrase. La coml\'exit\'e de cet
algorithme est de $O(n^3)$. \par

Le parseur bas\'e sur l'algorithm CKY attend en entr\'ee une grammaire sous forme
normale de Chomsky et une phrase (tok\'enis\'ee) \`a analyser. La phrase peut \^etre
d\'ej\`a \'etiquet\'ee, dans ce cas c'est une s\'equence de cat\'egories lexicales qui est
pass\'ee \`a l'algorithme.

\subsubsection{L'algorithme standard}
Dans la version classique de l'algorithme CKY, l'algorithme parcourt la phrase
donn\'ee en analysant toutes ses sous-cha\^ines possibles, en commencant par les
sous-chaines de longueur 1 (donc des mots de la phrase) et en remontant vers la
plus grande sous-cha\^ine qui est la phrase elle-m\^eme. L'algorithme
part toujours de la partie droite de la r\`egle et cherche dand la
grammaire toutes les parties gauches correspondantes. \par

L'algorithme CKY peut
\^etre graphiquement repr\'esent\'ee par un tableau bidimensionnel carr\'e avec la longueur de cot\'e \'egale \`a la longueur de la phrase (\'egalement appell\'e la charte CKY (CKY chart)).
Seule une moiti\'e des cases du tableau est utilis\'ee. Chaque case du tableau correspond \`a une sous-cha\^ine de la phrase.
L'algorithme \'ecrit dans chaque case les r\`egles de la grammaire qui pourraient
engendrer cette sous-cha\^ine. En remontant vers la case qui correspond \`a la
phrase enti\`ere, on obtient ainsi tous les arbres de d\'erivation possibles. Si
dans cette derni\`ere case il existe une d\'erivation possible \`a partir de l'axiome
de la grammaire, cela veut dire que cette phrase appartient au langage engendr\'e
par cette grammaire.\par

La grammaire utilis\'ee par l'algorithme CKY doit obligatoirement \^etre sous forme
normale de Chomsky (flexible), c'est-\`a-dire que la partie droite des r\`egles de
d\'erivation consiste soit d'un terminal, soit de deux non-terminaux. Les productions
singuli\`eres (les productions de la forme $A \rightarrow B$, o\`u $A,B \in V$) sont
autoris\'ees aussi dans la forme flexible. Cette obligation est
due au fait que pour chaque case, l'algorithme parcourt toutes les divisions possibles de la sous-cha\^ine
correspondante en deux parties. \par

En cas de productions singuli\`eres, plusieurs r\`egles sont gard\'ees dans la case
correspondante. Dans notre projet, nous avons impl\'ement\'e les r\`egles dans les
cases du tableau CKY sous la forme des arbres de d\'erivation : dans chaque case
nous avons tous les arbres de d\'erivation de la sous-cha\^ine correspondante. Cela
permet de pouvoir garde la trace de toute la d\'erivation de n'importe quelle
sous-cha\^ine en m\^eme temps que les r\`egles correspondantes. Les arbres \'etant construits
au fur et \`a m\'esure du d\'eroulement de l'algorithme de parsing, on est lib\'er\'e de la n\'ec\'essit\'e
de les reconstruire \`a l\'etape de la pr\'esentation du r\'esultat.

\subsubsection{L'injection des probabilit\'es dans l'algorithme}

La version probabiliste de l'algorithme CKY est une extension de la version
classique. La seule diff\'erence consiste du fait qu'une probabilit\'e est attribu\'ee
\`a chaque r\`egle de la grammaire et donc \`a chaque arbre de d\'erivation. La
probabilit\'e d'un arbre de d\'erivation est calcul\'ee comme suit :
$$P(tree) = P(rule) * P(left subtree) * P(right subtree)$$

Si la r\`egle correspondante est une production singuli\`ere ou une r\`egle lexicale
(production d'un terminal), les probabilit\'es des sous-arbres manquants sont mises
\`a 1. \par

L'injection des probabilit\'es dans l'algorithme CKY permet de d\'esambiguiser le
parsing en trouvant les d\'erivations les plus probables. En remontant dans la
charte CKY des mots s\'epar\'es vers la phrase en entier, les probabilit\'es des
r\`egles se multiplient, et les plus hautes probabilit\'es donnent des plus lourds
poids aux arbres correspondants. \par

Il faut noter que si \`a un bas niveau de
d\'erivation un arbre a plus de poids qu'un autre, ce ne sera pas forc\'ement
l'arbre le plus "lourd"  qui sera inclus dans l'arbre de d\'erivation le plus
lourd sur un niveau sup\'erieur. Par exemple, si dans la m\^eme case il y a un arbre
avec la r\`egle $NP \rightarrow X$ \`a la racine, avec une haute probabilit\'e, et un
autre arbre avec $VP \rightarrow X$, avec une probabilit\'e plus basse, alors que
dans une case plus haute de la charte il y a deux r\`egles correspondantes : $S
\rightarrow NP Y$ et $S \rightarrow VP Y$, la probabilit\'e de la deuxi\`eme r\`egle
\`etant beaucoup plus haute que celle de la premi\`ere, la diff\'erence de poids entre
les deux arbres peut \^etre neutralis\'ee par ces deux r\`egles. Cela justifie 
que si \`a la fin on veut ressortir plusieurs meilleurs arbres il
faut garder toutes les d\'erivations possibles dans chaque case de la charte, au
lieu d'en garder seulement la meilleure. En revanche, si ne garder qu'un seul arbre par case n'est 
 dans aucun pas appropri\'e pour la recherche de la meilleure analyse syntaxique,
 dans le cas o\'u seulement le meuilleur arbre est demand\'e 
 il est facile de voir qu'on peut quand m\^eme \'economiser sur le stockage de donn\'ees
 en ne gardant qu'une seule d\'erivation par chaque symb\^ole dans la case. \par

Dans la case au coin du tableau (la case qui correspond \`a la phrase enti\`ere) on
obtient alors toutes les d\'erivations possibles de la phrase (une seule si la
phrase n'est pas ambigue). En triant les arbres obtenus par ordre d\'ecroissant de
probabilit\'e, on obtient l'analyse syntaxique le plus probable (le plus
probablement choisie par un humain). Les k analyses avec les plus hautes
probabilit\'es (appel\'ees k-best parses) peuvent \^etre renvoy\'ees afin qu'un humain
puisse en choisir la bonne. Pour la cause d'optimisation \'a l'int\'erieur de chaque case
du tableau de parsing on maintient le tri des arbres par ordre croissant de leurs probabilit\'es
de fa\'con \`a ce qu'\`a l'\'etape final tous les arbres sont d\'ej\`a tri\'es et il suffit d'en prendre 
les $k$ premiers.


\subsection{La transformation de la grammaire probabiliste en forme normale de Chomsky flexible}

Comme dit ci-dessus, la grammaire doit \^etre en forme normale de Chomsky
(Chomsky Normal form, CNF) pour pouvoir \^etre utilis\'ee par le parseur CKY. Toute
grammaire hors contexte peut \^etre transform\'ee en CNF. \par

Dans une grammaire en
CNF, toutes le r\`egles ont la forme $$A \rightarrow BC$$ ou $$A \rightarrow
\alpha$$, o\`u $A,B,C \in V$ et $\alpha \in \Sigma$. \par

Une grammaire en CNF peut
aussi engendrer le mot vide : $$S \rightarrow \epsilon$$, o\`u S est l'axiome de
la grammaire. Par contre, d\`es lors que notre projet est ax\'e au traitement des langages naturels
qui eux ne contiennent pas le mots vide, ce cas n'est pas trait\'e par nos outils.
 
Dans notre projet, nous avons besoin de transformer la grammaire en forme
normale de Chomsky flexible (flexible CNF), qui autorise les productions
singuli\`eres de la forme $$A \rightarrow B$$. \par

Nous avons impl\'ement\'e la transformation d'une grammaire hors-contexte probabiliste en CNF en
module s\'epar\'e, qui est utilis\'e en pr\'e-traitement de la grammaire avant
de la passer au parseur. 

La proc\'edure de transformation de la grammaire en forme normal de Chomsky flexible
se fait par le biais de cr\'eation de nouveaux symb\^oles non-terminaux.
Pour cette transformation nous adoptons une convention de choix de noms (labels) pour ces non-terminaux
nouvels. On le fait de fa\'con \`a ce que ces non-terminaux puissent \^etre facilement reconnus et 
distingu\'es des non-terminaux de la grammaire d'origine. Par cons\'equent, quand on a besoin de reconvertir
les arbres syntaxiques issus de la grammaire en CNF aux arbres issus de la grammaire d'origine il nous suffit
d'enlever ces non-terminaux "artificiels" et de raccrocher leurs fils au noeud parent. Cette op\'eration est 
fait r\'ecursivement jusqu'\`a ce que tous les noeuds "artificiels" ne soient \'elimin\'es.
En ce qui concerne les probabilit\'es de nouvelles r\`egles ins\'er\'ees dans la grammaire prrobabiliste,
d\`es lors que lors de la tranformation de la grammaire on s'assure que chaque nouvel symb\^ole n'a qu'une
seule possibilit\'e de re\'ecriture chaque nouvelle r\`egle a une probabilit\'e \'egale \`a 1.

\subsection{Le traitement des mots inconnus}

Si on adopte la deuxi\`eme strat\'egie de l'analyse syntaxique, alors le
traitement des mots inconnus est laiss\'e au taggeur externe.
Le taggeur MElt r\'esout ce probl\`eme en se basant sur des lexiques externes tels
que Lefff \cite{Lefff}. Un mot qui n'a jamais \'et\'e rencontr\'e dans le corpus
d'entra\^inement est recherch\'e dans les lexiques. Donc une des solutions est
d'utiliser un dictionnaire externe (dans ce cas il faut tout de m\^eme prevoir
une mani\`ere suppl\'ementaire de g\'erer les mots inconnus au cas o\`u le mot n'existe
pas dans les lexiques utilis\'es).\par

Nous allons nous concentrer sur la r\'esolution de ce probl\`eme dans le cas de la
premi\`ere strat\'egie, quand c'est le parseur CKY probabiliste qui assigne les
cat\'egories morpho-syntaxiques aux entr\'ees lexicales. \par

Evidemment, si on estime
les probabilit\'es des r\`egles lexicales sur le corpus d'entrainement, alors
n'importe quelle phrase qui contient au moins un mot non-observ\'e dans ce corpus
va recevoir une probabilit\'e nulle, avec chaque arbre syntaxique de cette phrase
\'etant aussi de probabilit\'e nulle. Par cons\'equent, selon le choix
d'impl\'ementation soit le parseur probabiliste s'\'echouera, soit il enverra un
arbre syntaxique arbitraire (choisi parmi les arbres \'equiprobables). \par

Pour contourner cet effet ind\'esirable, il nous faut modifier notre proc\'edure
d'estimation de la PCFG de facon \`a ce que la probabilit\'e d'un mot inconnu
soit non-nulle. Pour le faire on a choisi de tester deux m\'ethodes diff\'erentes.

\subsubsection{Le traitement des mots inconnus: m\'ethode 1}

Mod\'elisons la probabilit\'e d'un mot $w$ sachant sa cat\'egorie morphosyntaxique
$C$ comme la probabilit\'e conjointe suivante :
\begin{eqnarray}
\label{prob_mot_inconnu_0}
 P(W = w \mid \tilde{C} = C) = P(W = w, \mathrm{UNKNOWN} = unknown \mid \tilde{C} = C).
\end{eqnarray}
La valeur $unknown$ est \'egale \`a 1 si le mot $w$ a \'et\'e observ\'e dans le
corpus d'entrainement et 0 sinon. D\`es lors que la valeur de la variable
$\mathrm{UNKNOWN}$ est en d\'ependance directe de la valeur $w$, on a affaire \`a
une relation d'inclusion entre les \'ev\'enements: l'\'ev\'enement $W = w$ est
inclu dans l'\'ev\'enement $\mathrm{UNKNOWN} = unknown$, donc l'\'egalit\'e
\ref{prob_mot_inconnu_0} tient gr\^ace \`a l'axiome de th\'eorie des probabilit\'es
suivante :
$$ A \subset B \Rightarrow P(A) = P(A,B).$$

Adoptons que pour chaque cat\'egorie morphosyntaxique $C$ la probabilit\'e de
rencontrer un mot inconnu est toujours la m\^eme et est \'egale \`a $\alpha$:
$$ \forall C \text{--- cat\'egorie morphosyntaxique} \; P(\mathrm{UNKNOWN} = 1 \mid \tilde{C} = C) = \alpha.$$

Ensuite on va d\'ecomposer la probabilit\'e en \ref{prob_mot_inconnu_0} en
utilisant la d\'efinition de la probabilit\'e conditionnelle.
\begin{multline}
\label{prob_mot_inconnu_cond}
 P(W = w \mid \tilde{C} = C) = P(W = w, \mathrm{UNKNOWN} = unknown \mid
 \tilde{C} = C) = \\ P(\mathrm{UNKNOWN} = unknown \mid \tilde{C} = C) P(W = w \mid \mathrm{UNKNOWN} = unknown , \tilde{C} = C).
\end{multline}

 Si le mot $w$ a \'et\'e observ\'e dans le corpus d'entrainement, alors on a
\begin{multline}
\label{prob_mot_connu_1}
 P(W = w \mid \tilde{C} = C) = P(W = w, \mathrm{UNKNOWN} = 0 \mid \tilde{C} = C)
 = \\ P(\mathrm{UNKNOWN} = 0 \mid \tilde{C} = C) P(W = w \mid \mathrm{UNKNOWN} =
 0, \tilde{C} = C) = \\ (1 - \alpha) P(W = w \mid \mathrm{UNKNOWN} = 0, \tilde{C} = C).
\end{multline}
La probabilit\'e $P(W = w \mid \mathrm{UNKNOWN} = 0, \tilde{C} = C)$ est estim\'ee,
comme d'habitude, par la fr\'equence relative de la paire $(w, C)$ dans le corpus
d'entra\^inement:
\begin{eqnarray*}
\hat{P}(W = w \mid \mathrm{UNKNOWN} = 0, \tilde{C} = C) =
\frac{ct(C \rightarrow w)}{\sum\limits_{a \in \Sigma}{ct(C \rightarrow a)}}.
\end{eqnarray*}

Ensuite on va traiter tous les mots inconnus comme un seul mot sans faire la
distinction entre eux.
Par cons\'equent on peut supposer qu'on a pr\'etrait\'e notre corpus de
d\'eveloppement (ou de test) de facon \`a remplacer tous les mots inconnus par le
m\^eme mot $w_{unknown}$ (qui n'a jamais \'et\'e rencontr\'e dans le corpus
d'entra\^inement). Alors $$\forall C \; P(W = w_{unknown} \mid \mathrm{UNKNOWN} =
1, \tilde{C} = C) = 1$$ et la probabilit\'e d'un mot inconnu est calcul\'ee comme
suit :
\begin{multline}
\label{prob_mot_inconnu_1}
 P(W = w_{unknown} \mid \tilde{C} = C) = P(W = w_{unknown}, \mathrm{UNKNOWN} = 1
 \mid \tilde{C} = C) = \\ P(\mathrm{UNKNOWN} = 1 \mid \tilde{C} = C) P(W =
 w_{unknown} \mid \mathrm{UNKNOWN} = 1, \tilde{C} = C) = \\ P(\mathrm{UNKNOWN} =
 1 \mid \tilde{C} = C) = \alpha.
\end{multline}

\subsubsection{Le traitement des mots inconnus: m\'ethode 2}

L'id\'ee de la deuxi\`eme m\'ethode est de calculer la probabilit\'e de
produire un mot inconnu sachant sa cat\'egorie lexicale $P(UNKNOWN|CAT)$ pour
chaque cat\'egorie \`a partir du corpus d'entra\^inement.\par

Pour cela, il faut fixer une limite en-dessous de laquelle tout mot rencontr\'e dans le corpus est
consid\'er\'e rare, et en calculant les fr\'equences relatives nous le comptons comme
un mot inconnu. En r\'esultat, le mot "UNKNOWN" a un compte diff\'erent de z\'ero, qui
est \'egal \`a la somme des comptes de tous les mots rares dans le corpus. On peut
ensuite calculer les fr\'equences relatives comme si "UNKNOWN" \'etait un mot comme
les autres.
\par

Cette m\'ethode est pr\'ef\'erable, parce qu'elle tient compte de la
fr\'equence des mots rares (donc la probabilit\'e de produire un mot inconnu) pour
chaque cat\'egorie lexicale dans la langue naturelle. La probabilit\'e
de produire un mot inconnu n'est pas la m\^eme pour toutes les
cat\'egories, elle est plus grande pour les cat\'egories lexicales ouvertes et plus
petite pour les cat\'egories ferm\'ees.
Intuitivement, il y aura moins de pr\'epositions rares dans un corpus que des noms rares, donc $P(UNKNOWN|P) < P(UNKNOWN|N)$.
\par

Cette m\'ethode est particuli\`erement utile si nous avons un corpus avec des
textes bien diverses et pas uniformes.

\subsection{Evaluation}

L'analyse d'une phrase passe par beaucoup d'\'etapes, chacune de ces \'etapes peut
\^etre \'evalu\'ee afin de trouver des sources d'impr\'ecision.\par

Les \'etapes \`a \'evaluer :
\begin{itemize}
\item Etiquetage morpho-syntaxique (Tagging)
\item Analyse syntaxique (Parsing)
\end{itemize}


La sortie d'une \'etape d'analyse forme l'entr\'ee de l'\'etape suivante.

Les types d'entr\'ee :
\begin{itemize}
\item Texte segment\'e (tok\'enis\'e) non-annot\'e
\item Texte morpho-syntaxiquement \'etiquet\'e (tagg\'e)
\item Corpus arbor\'e (muni d'annotations syntaxiques)
\end{itemize}

Pour \'evaluer le \textbf{tagging}, il faut extraire la phrase avec les cat\'egories
lexicales (ou juste la s\'equence des cat\'egories lexicales) de l'arbre
correspondant du corpus arbor\'e et comparer les cat\'egories lexicales dans les
deux phrases.\par

Pour \'evaluer le \textbf{parsing}, il faut comparer la meilleure analyse donn\'ee
par le parseur \`a l'arbre correspondant du corpus arbor\'e (l'arbre gold).

Pour chaque \'etape, il faut calculer le pourcentage des r\'esultats identiques \`a
la r\'ef\'erence gold parmi tous les r\'esultats.


\section{R\'esultats}

\section{Utilisation}
\subsection{La structure modulaire}
Le projet consiste des trois modules ind\'ependants qui permettent d'apprendre une
grammaire \`a partir de Sequoia Tree Bank \cite{Sequoia} et de l'utiliser pour
analyser des phrases donn\'ees par l'utilisateur. Chaque module prend en
argument l'output du module pr\'ecedent. L'ordre d'application de ces trois
modules est le suivant :
\begin{enumerate}
  \item Le module TrainSetReader, \'etant donn\'e les fichiers de Sequoia Tree Bank,
  en construit une grammaire hors-contexte probabiliste;
  \item Le module CNFConverter prend la grammaire et la transforme en forme
  normale de Chomsky flexible;
  \item Le module CKYParser se base sur la grammaire apprise par TrainSetReader
  et transform\'ee par CNFConverter pour donner l'analyse syntaxique des phrases
  en francais.
\end{enumerate}

\subsection{TrainSetReader}
\subsubsection{Nom}

TrainSetReader --- G\'en\'erer une grammaire \`a partir d'un corpus d'apprentissage.

\subsubsection{Synopsis}

{\ttfamily
\begin{verbatim}
java -jar TrainSetReader.jar [-p precision] [-t nthreads] [-u
unknown_threshold] [-s unknown_label] [-l] [files...]
\end{verbatim}
}

\subsubsection{Description}

Le programme \texttt{TrainSetReader} lit un texte annot\'e en constituants
syntaxiques sous forme arbor\'ee, selon les conventions utilis\'ees par le Sequoia Tree Bank. Il en
g\'en\`ere ensuite une grammaire alg\'ebrique probabiliste, chaque r\`egle de r\'e\'ecriture
voyant sa probabilit\'e estim\'ee par un calcul de fr\'equence relative. Cette
grammaire est affich\'ee sur la sortie standard.

Si des fichiers d'entr\'ee sont sp\'ecifi\'es, le programme lira seulement ceux-ci. Si
aucun fichier d'entr\'ee n'est sp\'ecifi\'e, les corpora d'entra\^inement seront lus sur
l'entr\'ee standard.

\subsubsection{Options}

\begin{description}[style=nextline]
\item[\texttt{-p, --precision precision}] Sp\'ecifie un nombre de chiffres apr\`es
la virgule qui seront affich\'es avec chaque r\`egle de r\'e\'ecriture. Sa valeur par d\'efaut est
\texttt{10}.
\item[\texttt{-t, --nthreads threads}] Sp\'ecifie le nombre de fils d'ex\'ecution
qui seront ex\'ecut\'es en m\^eme temps par le programme.
\item[\texttt{-u, --unknown-threshold unknown\_threshold}] Sp\'ecifie le nombre
d'occurrences dans le corpus en-dessous duquel la partie droite d'une r\`egle de
r\'e\'ecriture sera consid\'er\'ee comme rare, et donc inconnue.
\item[\texttt{-s, --unknown-label unknown\_label}] Sp\'ecifie le label qui sera
attribu\'e aux symboles inconnus.
\item[\texttt{-l, --lexical}] Si cette option est sp\'ecifi\'ee, le programme
affichera la grammaire avec ses r\`egles lexicales (dont la partie droite est un
symbole terminal). Celles-ci ne sont pas affich\'ees par d\'efaut.
\end{description}

\subsection{CNFConverter}
\subsubsection{Nom}

CNFConverter --- Transformer une grammaire hors contexte en grammaire en forme
normale de Chomsky flexible.

\subsubsection{Synopsis}

{\ttfamily
\begin{verbatim}
python cnf-converter.py [-p prob] [-t term_droite]
\end{verbatim}
}

\subsubsection{Description}

Le programme \texttt{CNFConverter} lit une grammaire hors contexte, probabiliste
ou pas, une r\`egle par ligne, de l'entr\'ee standard.
Il la transforme ensuite en une grammaire en CNF, en autoisant les productions
singuli\`eres.

\subsubsection{Options}

\begin{description}[style=nextline]
\item[\texttt{-p, --prob}] Si cette option est sp\'ecifi\'ee, le programme consid\`ere
que la grammaire donn\'ee est probabiliste (chaque r\`egle de production est
pr\'ec\'ed\'e par sa probabilit\'e).
\item[\texttt{-t, --term\_droite}] Si cette option est sp\'ecifi\'ee, cela
indique que dans les r\`egles avec plusieurs symboles dans la partie
droite il peut y avoir des symboles terminaux.
\end{description}

\subsection{CKYParser}
\subsubsection{Nom}

CKYParser --- Effectue l'analyse syntaxique d'une phrase donn\'ee \`a partir d'une
grammaire en CNF.

\subsubsection{Synopsis}

{\ttfamily
\begin{verbatim}
java -jar CKYParser.jar [-l log_prob] [-k k_best] [-a apriori_unknown_prob] [-u
unknown_label] -g grammar_file [-o output_file] [-n non_lexical_input]
\end{verbatim}
}

\subsubsection{Description}

Le programme \texttt{CKYParser} prend en argument une grammaire hors contexte
en forme normale de Chomsky et renvoie les k meilleures analyses syntaxiques
pour une phrase donn\'ee en entr\'ee standard (si cette phrase appartent au langage
engendr\'e par la grammaire) en appliquant l'algorithme CKY \`a cette phrase.

\subsubsection{Options}

\begin{description}[style=nextline]
\item[\texttt{-l, --log-prob log\_prob}] Si cette option est sp\'ecifi\'ee, le
programme utilisera les log-probabilit\'es dans l'algorithme de parsing.
\item[\texttt{-k, --k-best k\_best}] Sp\'ecifie le nombre maximal des meilleurs
arbres qui seront renvoy\'es par le programme.
\item[\texttt{-a, --apriori-unknown-prob apriori\_unknown\_prob}] Sp\'ecifie la
probabilit\'e apriori \`a attribuer \`a la production d'un mot unconnu. Si cette option est sp\'ecifi\'ee, la
m\'ethode du traitement des mots inconnus est chang\'ee \`a APRIORI\_PROB (pour chaque
cat\'egorie morphosyntaxique, la probabilit\'e de rencontrer un mot inconnu est
toujours la m\^eme et diff\'erente de z\'ero).
\item[\texttt{-u, --unknown-label unknown\_label}] Sp\'ecifie le label qui sera
attribu\'e aux symboles inconnus. Si cette option est sp\'ecifi\'ee, la
m\'ethode du traitement des mots inconnus est chang\'ee \`a RARE (tous les mots rares
dans le corpus d'entra\^inement sont consid\'er\'es comme inconnus, et la probabilit\'e
de produire un mot inconnu est calcul\'ee pour chaque cat\'egorie morphosyntaxique
\`a partir des comptes de ces mots rares).
\item[\texttt{-g, --grammar-file grammar\_file}] Cet argument est obligatoire.
C'est le chemin vers le fichier qui contient une grammaire en forme normale de
Chomsky, une r\`egle par ligne.
\item[\texttt{-o, --output-file output\_file}] Sp\'ecifie le chemin vers le
fichier dans lequel le parseur \'ecrira son output. Si cet argument n'est pas
donn\'e, le parseur \'ecrit le r\'esultat sur la sortie standard.
\item[\texttt{-n, --non-lexical-input non\_lexical\_input}] Si cette option est
sp\'ecifi\'ee, le programme consid\`ere que toute phrase donn\'ee consiste seulement des
non-terminaux (cat\'egories lexicales au lieu des mots).
\end{description}


\begin{thebibliography}{9}
    \bibitem{Jurafsky}
    Jurafsky,~D., Martin,~J., 2009. \emph{Speech and Language Processing: An Introduction to Natural Language Processing, Speech Recognition, and Computational Linguistics.} 2nd edition. Prentice-Hall.
    \bibitem{Sequoia}
    Candito,~M., Seddah,~D.
    \emph{Le corpus Sequoia: annotation syntaxique et exploitation pour
    l'adaptation d'analyseur par pont lexical.} Actes de TALN'2012, Grenoble,
    France.
    \bibitem{proper_PCFG_estimation}
    Chi,~Z., Geman,~S., 1998.
    \emph{Estimation of probabilistic context-free grammars.}, Computational Linguistics, 24(2):299-305.
    \bibitem{Jurafsky-Manning}
    Jurafsky,~D., Manning,~C., 2012. \emph{Free online course on Natural
    Language Programming.}
    \bibitem{MElt}
    Denis,~P., Sagot,~B.,2012.
    \emph{Coupling an annotated corpus and a lexicon for state-of-the-art POS
    tagging.} In Language Resources and Evaluation 46:4, pp. 721-736,
    DOI 10.1007/s10579-012-9193-0.
    \bibitem{Lefff}
    Sagot,~B.,2010.
    \emph{The Lefff, a freely available and large-coverage morphological and
    syntactic lexicon for French.} In Proceedings of the 7th international
    conference on Language Resources and Evaluation (LREC 2010), Istanbul,
    Turkey.

\end{thebibliography}

\end{document}


